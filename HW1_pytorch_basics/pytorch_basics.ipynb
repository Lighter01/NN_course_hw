{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# %matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from itertools import islice\n",
    "from collections.abc import Sequence\n",
    "from copy import deepcopy\n",
    "import tqdm.notebook as tq\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "\n",
    "SEED = 81020204\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "rng = np.random.default_rng(seed=SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_train = pd.read_csv('datasets/train_x.csv', delimiter=',', index_col=0)\n",
    "    y_train = pd.read_csv('datasets/train_y.csv', delimiter=',', index_col=0)\n",
    "    X_test  = pd.read_csv('datasets/test_x.csv',  delimiter=',', index_col=0)\n",
    "except Exception:\n",
    "    print('No such files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = px.colors.qualitative.Plotly\n",
    "\n",
    "sns.palplot(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите функцию, которая моделирует один нейрон с сигмоидной активацией и реализует вычисление градиента для обновления весов и смещений нейрона. ``Функция должна принимать список векторов признаков, ассоциированные бинарные метки класса, начальные веса, начальное смещение, скорость обучения и количество эпох. Функция должна обновлять веса и смещение с помощью градиентного спуска (классической версии) на основе функции потерь NLL и возвращать обновленные веса, смещение и список значений NLL для каждой эпохи, округленное до четырех десятичных знаков.`` Проведите обучение на предоставленном наборе данных из задания 4 (для двух разных лет). Опционально сгенерируйте другие подходящие наборы данных. Опишите ваши результаты. Предоставленная функция будет также протестирована во время защиты ДЗ. Можно использовать только чистый torch (без использования autograd и torch.nn)\n",
    "\n",
    "- {*} Выберите другую функцию потерь, проведите обучение с ее помощью. Сгенерируйте датасеты, на которых будет видна разница между алгоритмами. Покажите, в каких случаях выбор влияет на обучение. (2 балла)\n",
    "- {*} Реализуйте один из следующих видов градиентного спуска: Stochastic Gradient Descent (SGD), Batch Gradient Descent, Mini-Batch Gradient Descent. Проведите эксперименты, покажите разницу в сходимости, сходимость в зависимости от формы поверхности. (2 балла)\n",
    "\n",
    "Пример входа:\n",
    "```{python} \n",
    "input: \n",
    "    features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], \n",
    "    labels = [1, 0, 0], \n",
    "    initial_weights = [0.1, -0.2], \n",
    "    initial_bias = 0.0, \n",
    "    learning_rate = 0.1, \n",
    "    epochs = 2\n",
    "        \n",
    "output: \n",
    "    updated_weights = [0.0808, -0.1916], updated_bias = -0.0214, mse_values = [0.2386, 0.2348]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0. Вспомогательные функции визуализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График разделающей поверхности (не используется)\n",
    "\n",
    "def plot_3d_decision_surface_plotly(X, y, weights: dict[str, list[float]] = {}, title=''):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Scatter plot of data points\n",
    "    for i, cls in enumerate(np.unique(y)):\n",
    "        cls_values = X[np.nonzero(y == cls)]\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=cls_values[:, 0], y=cls_values[:, 1], z=cls_values[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=palette[i], \n",
    "                size=8, \n",
    "                opacity=0.8,\n",
    "                line=dict(color='white', width=0)\n",
    "            )\n",
    "        ))\n",
    "\n",
    "    xlim, ylim = ([X[:, i].min(), X[:, i].max()] for i in range(2))\n",
    "    x_grid, y_grid = np.meshgrid(np.linspace(*xlim, num=2), np.linspace(*ylim, num=2))\n",
    "\n",
    "    if weights:\n",
    "        for name, w in weights.items():\n",
    "            z_grid = -(w[0] * x_grid + w[1] * y_grid + w[-1]) / w[2]\n",
    "            fig.add_trace(go.Surface(x=x_grid, y=y_grid, z=z_grid, opacity=0.5, name=name))\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        scene=dict(\n",
    "            xaxis_title='Feature 1',\n",
    "            yaxis_title='Feature 2',\n",
    "            zaxis_title='Feature 3'\n",
    "        ),\n",
    "        title=title\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "# Example usage\n",
    "# plot_3d_decision_surface_plotly(X, y, weights={'Model 1': [1, -1, 1, 0.5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График разделяющих поверхностей (не используется)\n",
    "\n",
    "def plot_3d_decision_surface_matplotlib(X, y, weights: dict[str, list[float]] = {}, title=''):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    print(len(X))\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='Set1', s=50, alpha=0.7)\n",
    "\n",
    "    xlim, ylim = ([X[:, i].min(), X[:, i].max()] for i in range(2))\n",
    "    x_grid, y_grid = np.meshgrid(np.linspace(*xlim, num=100), np.linspace(*ylim, num=100))\n",
    "\n",
    "    if weights:\n",
    "        for name, w in weights.items():\n",
    "            z_grid = -(w[0] * x_grid + w[1] * y_grid + w[-1]) / w[2]\n",
    "            ax.plot_surface(x_grid, y_grid, z_grid, alpha=0.3, rstride=100, cstride=100, label=name)\n",
    "    \n",
    "    ax.set_xlabel('Feature 1')\n",
    "    ax.set_ylabel('Feature 2')\n",
    "    ax.set_zlabel('Feature 3')\n",
    "    plt.title(title)\n",
    "    plt.legend(title='Loss')\n",
    "    plt.show()\n",
    "\n",
    "# plot_3d_decision_surface_matplotlib(X, y, weights={'Model 1': [1, -1, 1, 0.5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График разделающих прямых простых моделей с двумя параметрами и одним сдвигом\n",
    "\n",
    "# weights = {'name': [w_1, w_2, bias]}\n",
    "def plot_decision_bounds(X, y, weights: dict[str: list[float]] = {}, plot_params = {}, palette = palette, title=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if not plot_params:\n",
    "        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, palette='Set1', alpha=0.6, edgecolor=None, legend=True if len(weights) < 2 else False)\n",
    "    else:\n",
    "        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, legend=True if len(weights) < 2 else False, **plot_params)\n",
    "    xmin, xmax = min(X[:, 0]), max(X[:, 0])\n",
    "    ymin, ymax = min(X[:, 1]), max(X[:, 1])\n",
    "\n",
    "    if weights:\n",
    "        for i, (name, w) in enumerate(weights.items()):\n",
    "            xd = np.array([xmin, xmax])\n",
    "            yd = -(w[0] * xd + w[-1]) / w[1]\n",
    "            plt.plot(xd, yd, lw=1, ls='--', label=name, color='black' if len(weights) == 1 else palette[i])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(title='Class' if not weights else 'Loss')\n",
    "    plt.xlim(xmin, xmax)\n",
    "    plt.ylim(ymin, ymax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Графики значений функций потерь\n",
    "\n",
    "# TODO: make cut of loss values for models with more train epochs than other\n",
    "def plot_losses(loss: dict[str: list[float]], plot_engine: str = 'matplotlib', palette: list[hex] = None):\n",
    "    df_tmp = pd.DataFrame(loss)\n",
    "    df_tmp['Epoch'] = df_tmp.index + 1\n",
    "\n",
    "    if plot_engine == 'matplotlib':\n",
    "\n",
    "        df_melted = df_tmp.melt(id_vars=['Epoch'], var_name='Model', value_name='Loss')\n",
    "\n",
    "        # sns.set(style=\"whitegrid\")\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.lineplot(data=df_melted, x='Epoch', y='Loss', hue='Model', palette=palette)\n",
    "\n",
    "        plt.title('Loss Convergence Across Models')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss Value')\n",
    "        plt.legend(title='Models')\n",
    "        plt.grid(True, axis='both', linestyle='--', color='gray', linewidth=0.5)\n",
    "\n",
    "        # sns.reset_defaults()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    elif plot_engine == 'plotly':\n",
    "        fig = go.Figure()\n",
    "\n",
    "        for model_name in loss:\n",
    "            fig.add_trace(go.Scatter(x=df_tmp['Epoch'], y=df_tmp[model_name], mode='lines',\n",
    "                                    name=model_name, marker=dict(size=8), line=dict(width=2)))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='Loss Convergence Across Models',\n",
    "            xaxis_title='Epoch',\n",
    "            yaxis_title='Loss Value',\n",
    "            legend_title='Models',\n",
    "            template='plotly_white',\n",
    "            font=dict(family=\"Arial\", size=14, color=\"black\"),\n",
    "            plot_bgcolor='#f0f0f0',\n",
    "            xaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray'),\n",
    "            yaxis=dict(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Подготовка данных из 4 задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.year.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsamp_mask = y_train.year.isin([2006, 2007])\n",
    "y_train_subsamp = y_train[subsamp_mask]\n",
    "X_train_subsamp = X_train[subsamp_mask]\n",
    "X_train_subsamp.shape, y_train_subsamp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_subsamp_train, X_train_subsamp_test, y_train_subsamp_train, y_train_subsamp_test = (\n",
    "    train_test_split(X_train_subsamp, y_train_subsamp, test_size=0.25, shuffle=True, random_state=SEED)\n",
    ")\n",
    "X_train_subsamp_train.shape, X_train_subsamp_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_subsamp_train = std_scaler.fit_transform(X_train_subsamp_train)\n",
    "X_train_subsamp_test  = std_scaler.transform(X_train_subsamp_test)\n",
    "X_train_subsamp_train[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_subsamp_train = label_encoder.fit_transform(y_train_subsamp_train.values.ravel())\n",
    "y_train_subsamp_test = label_encoder.transform(y_train_subsamp_test.values.ravel())\n",
    "y_train_subsamp_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Генерация другой задачи классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=10000, \n",
    "                           n_features=2,\n",
    "                           n_informative=2, \n",
    "                           n_redundant=0, \n",
    "                           n_classes=2,\n",
    "                           flip_y=0.1,\n",
    "                           class_sep=2,\n",
    "                           n_clusters_per_class=1,\n",
    "                           random_state=SEED, \n",
    "                           shuffle=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_bounds(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Numpy class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[BCE == NLL для бинарной классификации](https://en.wikipedia.org/wiki/Loss_functions_for_classification#Logistic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpy():\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 tolerance: float = 1e-4,\n",
    "                 early_stop: bool = False,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        self.weights = np.array(initial_weights).ravel() if not isinstance(initial_weights, np.ndarray) else initial_weights\n",
    "        self.bias = np.array(initial_bias).ravel() if not isinstance(initial_bias, np.ndarray) else initial_bias\n",
    "        self.tolerance = tolerance\n",
    "        self.early_stop = early_stop\n",
    "        self.n_startup_rounds = n_startup_rounds\n",
    "        self.early_stop_rounds = early_stop_rounds\n",
    "        self.random_seed = random_seed\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def fit(self, \n",
    "            features: list[list[float]], \n",
    "            labels: list[int], \n",
    "            learning_rate: float = 1e-3, \n",
    "            epochs: int = 100,\n",
    "            return_weights_history: bool = False) -> None | list[list[float]]:\n",
    "\n",
    "        X = np.array(features).squeeze() if not isinstance(features, np.ndarray) else deepcopy(features)\n",
    "        X = self.add_ones(X)\n",
    "        y = np.array(labels).squeeze() if not isinstance(labels, np.ndarray) else deepcopy(labels)\n",
    "\n",
    "        self.init_weights(X)\n",
    "        \n",
    "        w = np.hstack([self.bias, self.weights])\n",
    "\n",
    "        loss_values = []\n",
    "        if return_weights_history:\n",
    "            weights_values = []\n",
    "            weights_values.append(w.copy())\n",
    "            \n",
    "        no_improvement_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X, w)\n",
    "            loss = self.loss_fn(y, y_pred)\n",
    "\n",
    "            if self.early_stop:\n",
    "                if epoch > self.n_startup_rounds and 0 < (loss_values[-1] - loss) < self.tolerance:\n",
    "                    no_improvement_counter += 1\n",
    "                    if no_improvement_counter >= self.early_stop_rounds:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "                else:\n",
    "                    no_improvement_counter = 0\n",
    "\n",
    "            loss_values.append(loss)\n",
    "            if return_weights_history:\n",
    "                weights_values.append(w.copy())\n",
    "            \n",
    "            grad = self.gradient(X, y, y_pred)\n",
    "            w -= learning_rate * grad\n",
    "\n",
    "        self.weights = w[1:]\n",
    "        self.bias = w[0]\n",
    "        self.loss_values = loss_values\n",
    "\n",
    "        if return_weights_history:\n",
    "            return np.array(weights_values)\n",
    "\n",
    "\n",
    "    def predict(self, features: list[list[float]]):\n",
    "        X = np.array(features).squeeze() if not isinstance(features, np.ndarray) else deepcopy(features)\n",
    "        X = self.add_ones(X)\n",
    "\n",
    "        w = np.hstack([self.bias, self.weights])\n",
    "\n",
    "        probs = self.forward(X, w)\n",
    "\n",
    "        return np.where(probs >= 0.5, 1, 0)\n",
    "    \n",
    "\n",
    "    def add_ones(self, x):\n",
    "        return np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "    \n",
    "    def init_weights(self, x):\n",
    "        rng_ = np.random.default_rng(seed=self.random_seed)\n",
    "        if self.weights.size == 0:\n",
    "            self.weights = rng_.standard_normal(x.shape[1] - 1, dtype=np.float32)\n",
    "        if self.bias.size == 0:\n",
    "            self.bias = rng_.standard_normal(1, dtype=np.float32)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        if x >= 0:\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        else:\n",
    "            return np.exp(x) / (1 + np.exp(x))\n",
    "    \n",
    "    def forward(self, x, w):\n",
    "        logits = np.dot(x, w)\n",
    "        return np.array([self.sigmoid(value) for value in logits]) # probabilities\n",
    "    \n",
    "    def loss_fn(self, y_true, y_pred):\n",
    "        y_pos_loss = y_true * np.log(y_pred + self.eps)\n",
    "        y_neg_loss = (1 - y_true) * np.log(1 - y_pred + self.eps)\n",
    "        return -np.mean(y_pos_loss + y_neg_loss)\n",
    "    \n",
    "    def gradient(self, x, y_true, y_pred):\n",
    "        return np.dot((y_pred - y_true), x) / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "log_reg_numpy = LogRegNumpy(initial_weights=initial_weights, initial_bias=initial_bias)\n",
    "log_reg_numpy.fit(features=features,\n",
    "                  labels=labels,\n",
    "                  learning_rate=learning_rate,\n",
    "                  epochs=epochs)\n",
    "\n",
    "log_reg_numpy.weights, log_reg_numpy.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy = LogRegNumpy()\n",
    "\n",
    "log_reg_numpy.fit(features = X_train_subsamp_train, \n",
    "                  labels = y_train_subsamp_train, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 5)\n",
    "\n",
    "log_reg_numpy.weights, log_reg_numpy.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train_subsamp_train, log_reg_numpy.predict(X_train_subsamp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy = LogRegNumpy(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_numpy.fit(features = X, \n",
    "                  labels = y, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 10000)\n",
    "\n",
    "log_reg_numpy.weights, log_reg_numpy.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_numpy.weights, np.array([log_reg_numpy.bias])))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)\n",
    "accuracy_score(y, log_reg_numpy.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Numpy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_single_input(x: float):\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1 + np.exp(x))\n",
    "    \n",
    "def sigmoid(x: list[float]):\n",
    "    return np.array([sigmoid_single_input(value) for value in x])\n",
    "\n",
    "def nll_loss(probs, labels):\n",
    "    y_pos_loss = labels * np.log(probs + eps)\n",
    "    y_neg_loss = (1 - labels) * np.log(1 - probs + eps)\n",
    "    return -np.mean(y_pos_loss + y_neg_loss)\n",
    "\n",
    "# def nll_loss(probs, labels):\n",
    "#     return -(np.dot(labels, np.log(probs)) + np.dot(1 - labels, np.log(1 - probs))) / probs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_neuron(features: list[list[float]], \n",
    "                  labels: list[int], \n",
    "                  initial_weights: list[float],\n",
    "                  initial_bias: list[float],\n",
    "                  learning_rate: float,\n",
    "                  epochs: int,\n",
    "                  scorer: callable = None) -> Sequence[list[float], list[float], list[float]]:\n",
    "    features_np = np.array(features, dtype=np.float32)\n",
    "    labels_np = np.array(labels, dtype=np.int64)\n",
    "    weights = np.array(initial_weights, dtype=np.float32)\n",
    "    bias = np.array(initial_bias, dtype=np.float32)\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logits = np.dot(features_np, weights) + bias\n",
    "        probs = sigmoid(logits)\n",
    "        loss = nll_loss(probs, labels_np)\n",
    "\n",
    "        loss_values.append(loss)\n",
    "        \n",
    "        # GD\n",
    "        diff = np.subtract(probs, labels_np)\n",
    "        dw = np.dot(diff, features_np) / features_np.shape[0]\n",
    "        db = diff.mean()\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    if scorer is not None:\n",
    "        preds = [1 if p >= 0.5 else 0 for p in probs]\n",
    "        score = scorer(labels_np, preds)\n",
    "        print(f'{scorer.__name__}: {score:.4f}')\n",
    "\n",
    "    return np.round(weights, 4), np.round(bias, 4), np.round(loss_values, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron(features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], \n",
    "              labels = [1, 0, 0], \n",
    "              initial_weights = [0.1, -0.2], \n",
    "              initial_bias = 0.0, \n",
    "              learning_rate = 0.1, \n",
    "              epochs = 2,\n",
    "              scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = rng.standard_normal(X_train_subsamp_train.shape[1] + 1, dtype=np.float32)\n",
    "\n",
    "single_neuron(features = X_train_subsamp_train, \n",
    "              labels = y_train_subsamp_train, \n",
    "              initial_weights = init_weights[:-1], \n",
    "              initial_bias = init_weights[-1], \n",
    "              learning_rate = 1e-3, \n",
    "              epochs = 5,\n",
    "              scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Pytorch class manual (single gradient formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegTorch():\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 gen_machine: str = 'torch',\n",
    "                 tolerance: float = 1e-4,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        self.weights = (torch.tensor(initial_weights, dtype=torch.float32).ravel() \n",
    "                        if not isinstance(initial_weights, torch.FloatTensor) else initial_weights)\n",
    "        self.bias = (torch.tensor(initial_bias, dtype=torch.float32).ravel() \n",
    "                     if not isinstance(initial_bias, torch.FloatTensor) else initial_bias)\n",
    "        self.eps = 1e-9\n",
    "        self.gen_machine = gen_machine\n",
    "        self.tolerance = tolerance\n",
    "        self.n_startup_rounds = n_startup_rounds\n",
    "        self.early_stop_rounds = early_stop_rounds\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            features: list[list[float]], \n",
    "            labels: list[int], \n",
    "            learning_rate: float = 1e-3, \n",
    "            epochs: int = 100):\n",
    "\n",
    "        X = (torch.tensor(features, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(features, torch.FloatTensor) else deepcopy(features))\n",
    "        X = self.add_ones(X)\n",
    "        y = (torch.tensor(labels, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(labels, torch.FloatTensor) else deepcopy(labels))\n",
    "\n",
    "        self.init_weights(X)\n",
    "        \n",
    "        w = torch.hstack([self.bias, self.weights])\n",
    "\n",
    "        loss_values = []\n",
    "        no_improvement_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print(f'Epoch {epoch + 1}')\n",
    "            y_pred = self.forward(X, w)\n",
    "            loss = self.loss_fn(y, y_pred)\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch > self.n_startup_rounds and 0 < (loss_values[-1] - loss) < self.tolerance:\n",
    "                no_improvement_counter += 1\n",
    "                if no_improvement_counter >= self.early_stop_rounds:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_counter = 0\n",
    "\n",
    "            loss_values.append(loss)\n",
    "            \n",
    "            grad = self.gradient(X, y, y_pred)\n",
    "            w -= learning_rate * grad\n",
    "\n",
    "        self.weights = w[1:]\n",
    "        self.bias = w[0]\n",
    "        self.loss_values = loss_values\n",
    "\n",
    "\n",
    "    def predict(self, features: list[list[float]]):\n",
    "        X = (torch.tensor(features, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(features, torch.FloatTensor) else deepcopy(features))\n",
    "        X = self.add_ones(X)\n",
    "\n",
    "        w = torch.hstack([self.bias, self.weights])\n",
    "\n",
    "        probs = self.forward(X, w)\n",
    "\n",
    "        return torch.where(probs >= 0.5, 1, 0)\n",
    "    \n",
    "\n",
    "    def add_ones(self, x):\n",
    "        return torch.hstack([torch.ones(x.shape[0], 1), x])\n",
    "    \n",
    "    def init_weights(self, X):\n",
    "        if self.gen_machine == 'torch':\n",
    "\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(self.random_seed)\n",
    "            if self.weights.nelement() == 0:\n",
    "                self.weights = torch.normal(0, 1, size=(X.shape[1] - 1, ), generator=generator, dtype=torch.float32)\n",
    "            if self.bias.nelement() == 0:\n",
    "                self.bias = torch.normal(0, 1, size=(1,), generator=generator, dtype=torch.float32)\n",
    "\n",
    "        elif self.gen_machine == 'numpy':\n",
    "\n",
    "            rng_ = np.random.default_rng(seed=self.random_seed)\n",
    "            if self.weights.nelement() == 0:\n",
    "                self.weights = torch.tensor(rng_.standard_normal(X.shape[1] - 1, dtype=np.float32), dtype=torch.float32)\n",
    "            if self.bias.nelement() == 0:\n",
    "                self.bias = torch.tensor(rng_.standard_normal(1, dtype=np.float32), dtype=torch.float32)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        if x >= 0:\n",
    "            return 1 / (1 + torch.exp(-x))\n",
    "        else:\n",
    "            return torch.exp(x) / (1 + torch.exp(x))\n",
    "    \n",
    "    def forward(self, x, w):\n",
    "        logits = torch.matmul(x, w)\n",
    "        return torch.tensor([self.sigmoid(value) for value in logits], dtype=torch.float32) # probabilities\n",
    "    \n",
    "    def loss_fn(self, y_true, y_pred):\n",
    "        y_pos_loss = y_true * torch.log(y_pred + self.eps)\n",
    "        y_neg_loss = (1 - y_true) * torch.log(1 - y_pred + self.eps)\n",
    "        return -torch.mean(y_pos_loss + y_neg_loss)\n",
    "    \n",
    "    def gradient(self, x, y_true, y_pred):\n",
    "        return torch.matmul((y_pred - y_true), x) / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "log_reg_torch = LogRegTorch(initial_weights=initial_weights, initial_bias=initial_bias)\n",
    "log_reg_torch.fit(features=features,\n",
    "                  labels=labels,\n",
    "                  learning_rate=learning_rate,\n",
    "                  epochs=epochs)\n",
    "\n",
    "log_reg_torch.weights, log_reg_torch.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_torch = LogRegTorch(gen_machine='numpy')\n",
    "\n",
    "log_reg_torch.fit(features = X_train_subsamp_train, \n",
    "                  labels = y_train_subsamp_train, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 5)\n",
    "\n",
    "log_reg_torch.weights, log_reg_torch.bias, log_reg_torch.loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train_subsamp_train, log_reg_torch.predict(X_train_subsamp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_torch = LogRegTorch(gen_machine='numpy', random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_torch.fit(features = X, \n",
    "                  labels = y, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 200)\n",
    "\n",
    "log_reg_torch.weights, log_reg_torch.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_torch.weights, np.array([log_reg_torch.bias])))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)\n",
    "accuracy_score(y, log_reg_torch.predict(X).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Pytorch function (single gradient formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_single_input_pt(x: float):\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "    else:\n",
    "        return torch.exp(x) / (1 + torch.exp(x))\n",
    "    \n",
    "def sigmoid_pt(x: list[float]):\n",
    "    return torch.tensor([sigmoid_single_input_pt(value) for value in x])\n",
    "\n",
    "def nll_loss_pt(probs, labels):\n",
    "    y_pos_loss = labels * torch.log(probs + eps)\n",
    "    y_neg_loss = (1 - labels) * torch.log(1 - probs + eps)\n",
    "    return -torch.mean(y_pos_loss + y_neg_loss)\n",
    "\n",
    "# def nll_loss_pt(probs, labels):\n",
    "#     return -(torch.dot(labels, torch.log(probs + eps)) + torch.dot(1 - labels, torch.log(1 - probs + eps))) / probs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_neuron_pt(features: list[list[float]], \n",
    "                     labels: list[int], \n",
    "                     initial_weights: list[float],\n",
    "                     initial_bias: list[float],\n",
    "                     learning_rate: float,\n",
    "                     epochs: int,\n",
    "                     scorer: callable = None) -> Sequence[list[float], list[float], list[float]]:\n",
    "    # requires_grad = False по дефолту\n",
    "    features_pt = torch.tensor(features, dtype=torch.float32)\n",
    "    labels_pt = torch.tensor(labels, dtype=torch.float32)\n",
    "    if not torch.is_tensor(initial_weights):\n",
    "        weights = torch.tensor(initial_weights, dtype=torch.float32)\n",
    "        bias = torch.tensor(initial_bias, dtype=torch.float32)\n",
    "    else:\n",
    "        weights = initial_weights.clone().detach()\n",
    "        bias = initial_bias.clone().detach()\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logits = torch.matmul(features_pt, weights) + bias\n",
    "        probs = sigmoid_pt(logits)\n",
    "        loss = nll_loss_pt(probs, labels_pt)\n",
    "\n",
    "        loss_values.append(torch.round(loss, decimals=4))\n",
    "        \n",
    "        # GD\n",
    "        diff = probs - labels_pt\n",
    "        dw = torch.matmul(diff, features_pt) / features_pt.shape[0]\n",
    "        db = diff.mean()\n",
    "        weights -= learning_rate * dw\n",
    "        bias -= learning_rate * db\n",
    "\n",
    "    if scorer is not None:\n",
    "        preds = [1 if p >= 0.5 else 0 for p in probs]\n",
    "        score = scorer(labels_pt.numpy(), preds)\n",
    "        print(f'{scorer.__name__}: {score:.4f}')\n",
    "\n",
    "    return torch.round(weights, decimals=4), torch.round(bias, decimals=4), torch.tensor(loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron_pt(features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], \n",
    "                labels = [1, 0, 0], \n",
    "                initial_weights = [0.1, -0.2], \n",
    "                initial_bias = 0.0, \n",
    "                learning_rate = 0.1, \n",
    "                epochs = 2,\n",
    "                scorer=accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_weights_tensor = torch.empty(X_train_subsamp_train.shape[1] + 1)\n",
    "# nn.init.uniform_(init_weights_tensor, -5, 5)\n",
    "init_weights_tensor = torch.tensor(init_weights, dtype=torch.float32)\n",
    "\n",
    "single_neuron_pt(features = X_train_subsamp_train, \n",
    "                labels = y_train_subsamp_train, \n",
    "                initial_weights = init_weights_tensor[:-1], \n",
    "                initial_bias = init_weights_tensor[-1], \n",
    "                learning_rate = 1e-3, \n",
    "                epochs = 5,\n",
    "                scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Numpy class (chain rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpyChainRule(LogRegNumpy):\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 tolerance: float = 1e-4,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        super(LogRegNumpyChainRule, self).__init__(initial_weights, initial_bias, tolerance, \n",
    "                                                   n_startup_rounds, early_stop_rounds, random_seed)\n",
    "    \n",
    "    def gradient(self, x, y_true, y_pred):\n",
    "        # dL_dp = -(labels_np / (probs + eps)) + ((1 - labels_np) / (1 - probs + eps))\n",
    "        dL_dp = (y_pred - y_true) / (y_pred  * (1 - y_pred) + self.eps)\n",
    "        dp_dz = y_pred * (1 - y_pred)\n",
    "        dL_dz = dL_dp * dp_dz\n",
    "        dL_dw = np.dot(dL_dz, x) / x.shape[0]\n",
    "        return dL_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "log_reg_numpy_chain_rule = LogRegNumpyChainRule(initial_weights=initial_weights, initial_bias=initial_bias)\n",
    "log_reg_numpy_chain_rule.fit(features=features,\n",
    "                            labels=labels,\n",
    "                            learning_rate=learning_rate,\n",
    "                            epochs=epochs)\n",
    "\n",
    "log_reg_numpy_chain_rule.weights, log_reg_numpy_chain_rule.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_chain_rule = LogRegNumpyChainRule()\n",
    "\n",
    "log_reg_numpy_chain_rule.fit(features = X_train_subsamp_train, \n",
    "                            labels = y_train_subsamp_train, \n",
    "                            learning_rate = 1e-3, \n",
    "                            epochs = 5)\n",
    "\n",
    "log_reg_numpy_chain_rule.weights, log_reg_numpy_chain_rule.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train_subsamp_train, log_reg_numpy_chain_rule.predict(X_train_subsamp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_chain_rule = LogRegNumpyChainRule(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_numpy_chain_rule.fit(features = X, \n",
    "                             labels = y, \n",
    "                             learning_rate = 1e-3, \n",
    "                             epochs = 10000)\n",
    "\n",
    "log_reg_numpy_chain_rule.weights, log_reg_numpy_chain_rule.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_numpy_chain_rule.weights, np.array([log_reg_numpy_chain_rule.bias])))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)\n",
    "accuracy_score(y, log_reg_numpy_chain_rule.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Numpy function (chain rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_neuron_sequential_grad(features: list[list[float]], \n",
    "                                  labels: list[int], \n",
    "                                  initial_weights: list[float],\n",
    "                                  initial_bias: list[float],\n",
    "                                  learning_rate: float,\n",
    "                                  epochs: int,\n",
    "                                  scorer: callable = None) -> Sequence[list[float], list[float], list[float]]:\n",
    "    features_np = np.array(features, dtype=np.float32)\n",
    "    labels_np = np.array(labels, dtype=np.int64)\n",
    "    weights = np.array(initial_weights, dtype=np.float32)\n",
    "    bias = np.array(initial_bias, dtype=np.float32)\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logits = np.dot(features_np, weights) + bias\n",
    "        probs = sigmoid(logits)\n",
    "        loss = nll_loss(probs, labels_np)\n",
    "\n",
    "        loss_values.append(loss)\n",
    "        \n",
    "        # GD\n",
    "        # dL_dp = -(labels_np / (probs + eps)) + ((1 - labels_np) / (1 - probs + eps))\n",
    "        dL_dp = (probs - labels_np) / (probs  * (1 - probs) + eps)\n",
    "        dp_dz = probs * (1 - probs)\n",
    "        dL_dz = dL_dp * dp_dz\n",
    "        dL_dw = np.dot(dL_dz, features_np) / features_np.shape[0]\n",
    "        dL_db = np.mean(dL_dz)\n",
    "\n",
    "        weights -= learning_rate * dL_dw\n",
    "        bias -= learning_rate * dL_db\n",
    "\n",
    "    if scorer is not None:\n",
    "        preds = np.where(probs >= 0.5, 1, 0)\n",
    "        score = scorer(labels, preds)\n",
    "        print(f'{scorer.__name__}: {score:.4f}')\n",
    "\n",
    "    return np.round(weights, 4), np.round(bias, 4), np.round(loss_values, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron_sequential_grad(features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], \n",
    "                              labels = [1, 0, 0], \n",
    "                              initial_weights = [0.1, -0.2], \n",
    "                              initial_bias = 0.0, \n",
    "                              learning_rate = 0.1, \n",
    "                              epochs = 2,\n",
    "                              scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron_sequential_grad(features = X_train_subsamp_train, \n",
    "              labels = y_train_subsamp_train, \n",
    "              initial_weights = init_weights[:-1], \n",
    "              initial_bias = init_weights[-1], \n",
    "              learning_rate = 1e-3, \n",
    "              epochs = 5,\n",
    "              scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pytorch class (autograd exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegTorchAutograd():\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 gen_machine: str = 'torch',\n",
    "                 tolerance: float = 1e-4,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        self.weights = (torch.tensor(initial_weights, dtype=torch.float32).ravel() \n",
    "                        if not isinstance(initial_weights, torch.FloatTensor) else initial_weights)\n",
    "        self.bias = (torch.tensor(initial_bias, dtype=torch.float32).ravel() \n",
    "                     if not isinstance(initial_bias, torch.FloatTensor) else initial_bias)\n",
    "        self.gen_machine = gen_machine\n",
    "        self.tolerance = tolerance\n",
    "        self.n_startup_rounds = n_startup_rounds\n",
    "        self.early_stop_rounds = early_stop_rounds\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            features: list[list[float]], \n",
    "            labels: list[int], \n",
    "            learning_rate: float = 1e-3, \n",
    "            epochs: int = 100):\n",
    "\n",
    "        X = (torch.tensor(features, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(features, torch.FloatTensor) else deepcopy(features))\n",
    "        y = (torch.tensor(labels, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(labels, torch.FloatTensor) else deepcopy(labels))\n",
    "\n",
    "        self.init_weights(X)\n",
    "        \n",
    "        self.linear = nn.Linear(X.shape[1], 1)\n",
    "        # Можно вместо такого костыля воспользоваться nn.Parameter\n",
    "        self.linear.weight.data = self.weights.view(1, -1)\n",
    "        self.linear.bias.data = self.bias.view(1)\n",
    "        loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        loss_values = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.linear(X).squeeze()\n",
    "            loss = loss_function(y_pred, y)\n",
    "\n",
    "            no_improvement_counter = 0\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch > self.n_startup_rounds and 0 < (loss_values[-1] - loss) < self.tolerance:\n",
    "                no_improvement_counter += 1\n",
    "                if no_improvement_counter >= self.early_stop_rounds:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_counter = 0\n",
    "\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.linear.weight -= learning_rate * self.linear.weight.grad\n",
    "                self.linear.bias -= learning_rate * self.linear.bias.grad\n",
    "\n",
    "            # Интересно, что я изначально забыл перенести в класс зануление градиентов.\n",
    "            # При этом модель с накопленными градиентами смогла сойтись в примере 3.\n",
    "            # А с занулением она никак не может этого сделать.\n",
    "            self.linear.weight.grad.zero_()\n",
    "            self.linear.bias.grad.zero_()\n",
    "\n",
    "        self.weights = self.linear.weight.data.squeeze()\n",
    "        self.bias = self.linear.bias.data.squeeze()\n",
    "        self.loss_values = loss_values\n",
    "\n",
    "\n",
    "    def predict(self, features: list[list[float]]):\n",
    "        X = (torch.tensor(features, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(features, torch.FloatTensor) else deepcopy(features))\n",
    "\n",
    "        probs = self.linear(X).squeeze()\n",
    "\n",
    "        return torch.where(probs >= 0.5, 1, 0)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, X):\n",
    "        if self.gen_machine == 'torch':\n",
    "\n",
    "            generator = torch.Generator()\n",
    "            generator.manual_seed(self.random_seed)\n",
    "            if self.weights.nelement() == 0:\n",
    "                self.weights = torch.normal(0, 1, size=(X.shape[1], ), generator=generator, dtype=torch.float32)\n",
    "            if self.bias.nelement() == 0:\n",
    "                self.bias = torch.normal(0, 1, size=(1,), generator=generator, dtype=torch.float32)\n",
    "\n",
    "        elif self.gen_machine == 'numpy':\n",
    "\n",
    "            rng_ = np.random.default_rng(seed=self.random_seed)\n",
    "            if self.weights.nelement() == 0:\n",
    "                self.weights = torch.tensor(rng_.standard_normal(X.shape[1], dtype=np.float32), dtype=torch.float32)\n",
    "            if self.bias.nelement() == 0:\n",
    "                self.bias = torch.tensor(rng_.standard_normal(1, dtype=np.float32), dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "log_reg_torch_autograd = LogRegTorchAutograd(initial_weights=initial_weights, initial_bias=initial_bias)\n",
    "log_reg_torch_autograd.fit(features=features,\n",
    "                          labels=labels,\n",
    "                          learning_rate=learning_rate,\n",
    "                          epochs=epochs)\n",
    "\n",
    "log_reg_torch_autograd.weights, log_reg_torch_autograd.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_torch_autograd = LogRegTorchAutograd(gen_machine='numpy')\n",
    "\n",
    "log_reg_torch_autograd.fit(features = X_train_subsamp_train, \n",
    "                           labels = y_train_subsamp_train, \n",
    "                           learning_rate = 1e-3, \n",
    "                           epochs = 5)\n",
    "\n",
    "log_reg_torch_autograd.weights, log_reg_torch_autograd.bias, log_reg_torch_autograd.loss_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train_subsamp_train, log_reg_torch_autograd.predict(X_train_subsamp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_torch_autograd = LogRegTorchAutograd(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_torch_autograd.fit(features = X, \n",
    "                           labels = y, \n",
    "                           learning_rate = 1e-3, \n",
    "                           epochs = 20000)\n",
    "\n",
    "log_reg_torch_autograd.weights, log_reg_torch_autograd.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_torch_autograd.weights, np.array([log_reg_torch_autograd.bias])))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)\n",
    "accuracy_score(y, log_reg_torch_autograd.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Pytorch class (autograd exploitation 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegTorchAutograd2(LogRegTorchAutograd):\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 gen_machine: str = 'torch',\n",
    "                 tolerance: float = 1e-4,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        super(LogRegTorchAutograd2, self).__init__(initial_weights, initial_bias, gen_machine, tolerance,\n",
    "                                                   n_startup_rounds, early_stop_rounds, random_seed)\n",
    "\n",
    "\n",
    "    def fit(self, \n",
    "            features: list[list[float]], \n",
    "            labels: list[int], \n",
    "            learning_rate: float = 1e-3, \n",
    "            epochs: int = 100):\n",
    "\n",
    "        X = (torch.tensor(features, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(features, torch.FloatTensor) else deepcopy(features))\n",
    "        y = (torch.tensor(labels, dtype=torch.float32).squeeze() \n",
    "             if not isinstance(labels, torch.FloatTensor) else deepcopy(labels))\n",
    "\n",
    "        self.init_weights(X)\n",
    "        \n",
    "        self.linear = nn.Linear(X.shape[1], 1)\n",
    "        self.linear.weight.data = self.weights.view(1, -1)\n",
    "        self.linear.bias.data = self.bias.view(1)\n",
    "        loss_function = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "        loss_values = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.linear(X).squeeze()\n",
    "            loss = loss_function(y_pred, y)\n",
    "\n",
    "            no_improvement_counter = 0\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch > self.n_startup_rounds and 0 < (loss_values[-1] - loss) < self.tolerance:\n",
    "                no_improvement_counter += 1\n",
    "                if no_improvement_counter >= self.early_stop_rounds:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_counter = 0\n",
    "\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # changes here\n",
    "            weights_grad = torch.autograd.grad(loss, self.linear.weight, retain_graph=True)[0]\n",
    "            bias_grad = torch.autograd.grad(loss, self.linear.bias)[0]\n",
    "            with torch.no_grad():\n",
    "                self.linear.weight -= learning_rate * weights_grad\n",
    "                self.linear.bias -= learning_rate * bias_grad\n",
    "\n",
    "        self.weights = self.linear.weight.data.squeeze()\n",
    "        self.bias = self.linear.bias.data.squeeze()\n",
    "        self.loss_values = loss_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_torch_autograd_2 = LogRegTorchAutograd2(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_torch_autograd_2.fit(features = X, \n",
    "                             labels = y, \n",
    "                             learning_rate = 1e-4, \n",
    "                             epochs = 20000)\n",
    "\n",
    "log_reg_torch_autograd_2.weights, log_reg_torch_autograd_2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_torch_autograd_2.weights, np.array([log_reg_torch_autograd_2.bias])))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)\n",
    "accuracy_score(y, log_reg_torch_autograd_2.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Pytorch function (autograd exploitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_neuron_autograd(features: list[list[float]], \n",
    "                            labels: list[int], \n",
    "                            initial_weights: list[float],\n",
    "                            initial_bias: list[float],\n",
    "                            learning_rate: float,\n",
    "                            epochs: int,\n",
    "                            scorer: callable = None) -> Sequence[list[float], list[float], list[float]]:\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "    if not torch.is_tensor(initial_weights):\n",
    "        weights = torch.tensor(initial_weights, dtype=torch.float32)\n",
    "        bias = torch.tensor(initial_bias, dtype=torch.float32)\n",
    "    else:\n",
    "        weights = initial_weights.clone().detach()\n",
    "        bias = initial_bias.clone().detach()\n",
    "\n",
    "    linear = nn.Linear(features_tensor.shape[1], 1)\n",
    "    linear.weight.data = weights.view(1, -1)\n",
    "    linear.bias.data = bias.view(1)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "    loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        logits = linear(features_tensor).squeeze(1)\n",
    "        loss = loss_fn(logits, labels_tensor)\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        # GD\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            linear.weight -= learning_rate * linear.weight.grad\n",
    "            linear.bias -= learning_rate * linear.bias.grad\n",
    "\n",
    "        linear.weight.grad.zero_()\n",
    "        linear.bias.grad.zero_()\n",
    "\n",
    "    if scorer is not None:\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = torch.where(probs >= 0.5, 1, 0).numpy()\n",
    "        print(f'{scorer.__name__}: {scorer(labels, preds)}')\n",
    "\n",
    "    return (np.round(linear.weight.data.squeeze(0).tolist(), 4), \n",
    "            np.round(linear.bias.data.tolist(), 4), \n",
    "            np.round(loss_values, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron_autograd(features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]], \n",
    "                        labels = [1, 0, 0], \n",
    "                        initial_weights = [0.1, -0.2], \n",
    "                        initial_bias = 0.0, \n",
    "                        learning_rate = 0.1, \n",
    "                        epochs = 2,\n",
    "                        scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_neuron_autograd(features = X_train_subsamp_train, \n",
    "                       labels = y_train_subsamp_train, \n",
    "                       initial_weights = init_weights_tensor[:-1], \n",
    "                       initial_bias = init_weights_tensor[-1], \n",
    "                       learning_rate = 1e-3, \n",
    "                       epochs = 5,\n",
    "                       scorer = accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pytorch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, init_weights = None, init_bias = None):\n",
    "        super(LogisticRegressionNet, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        if init_weights is not None:\n",
    "            self.linear.weight.data = init_weights.reshape(output_dim, input_dim)\n",
    "        if init_bias is not None:\n",
    "            self.linear.bias.data = init_bias.reshape(output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = F.sigmoid(out) \n",
    "        # действительно, выглядит так, что использование F.sigmoid + BCELoss является\n",
    "        # менее вычислительно устойчивым/стабильным подходом, чем использование\n",
    "        # BCEWithLogitsLoss. Использование второго подхода дает схожие со всеми \n",
    "        # остальными методами значения потерь, тогда как явное использование сигмоиды\n",
    "        # приводит к завышенным значениям функции потерь\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor([[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]])\n",
    "labels = torch.tensor([1, 0, 0])\n",
    "initial_weights = torch.tensor([0.1, -0.2])\n",
    "initial_bias = torch.tensor([0.0])\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "logistic_regression_net = LogisticRegressionNet(features.shape[1], 1, initial_weights, initial_bias)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(logistic_regression_net.parameters(), lr=learning_rate, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_net_train(model, X, y, loss_fn, optimizer):\n",
    "\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs.squeeze(), y.float())\n",
    "\n",
    "        loss_values.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return (model.state_dict()['linear.weight'].squeeze(), \n",
    "            model.state_dict()['linear.bias'].squeeze(), \n",
    "            np.round(loss_values, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_net_train(logistic_regression_net, features, labels, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(X_train_subsamp_train, dtype=torch.float32)\n",
    "labels = torch.tensor(y_train_subsamp_train, dtype=torch.float32)\n",
    "initial_weights = init_weights_tensor[:-1]\n",
    "initial_bias = init_weights_tensor[-1]\n",
    "learning_rate = 1e-3\n",
    "epochs = 5\n",
    "\n",
    "logistic_regression_net = LogisticRegressionNet(features.shape[1], 1, initial_weights, initial_bias)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(logistic_regression_net.parameters(), lr=learning_rate, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_net_train(logistic_regression_net, features, labels, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) sigmoid + BCE: [-1.0791,  1.2970, -0.4337,  0.7704, -1.3520], Loss:  array([5.7214, 5.7211, 5.7208, 5.7204, 5.7202])\n",
    "\n",
    "2) BCEWithLogitsLoss: [-1.0776,  1.2973, -0.4331,  0.7684, -1.3524], Loss: array([3.6517, 3.6512, 3.6508, 3.6504, 3.6499])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    probs = logistic_regression_net(features).squeeze().numpy()\n",
    "    preds = np.where(probs >= 0.5, 1, 0)\n",
    "    print(f'Accuracy score: {accuracy_score(labels, preds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.tensor(X, dtype=torch.float32)\n",
    "labels = torch.tensor(y, dtype=torch.float32)\n",
    "learning_rate = 1e-3\n",
    "epochs = 20000\n",
    "\n",
    "logistic_regression_net = LogisticRegressionNet(features.shape[1], 1)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(logistic_regression_net.parameters(), lr=learning_rate, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = log_reg_net_train(logistic_regression_net, features, labels, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_net_weights = logistic_regression_net.state_dict()['linear.weight'].squeeze().numpy()\n",
    "log_reg_net_bias = logistic_regression_net.state_dict()['linear.bias'].squeeze().numpy()\n",
    "\n",
    "log_reg_net_weights, log_reg_net_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_net_weights, np.array([log_reg_net_bias])))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)\n",
    "\n",
    "probs = logistic_regression_net(features).detach().numpy().squeeze()\n",
    "preds = np.where(probs >= 0.5, 1, 0)\n",
    "accuracy_score(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SKlearn logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_logistic_regression = LogisticRegression(penalty=None, \n",
    "                                                 random_state=SEED,\n",
    "                                                 solver='lbfgs',\n",
    "                                                 tol=1e-5, \n",
    "                                                 max_iter=100, \n",
    "                                                 n_jobs=1)\n",
    "\n",
    "sklearn_logistic_regression.fit(X_train_subsamp_train, y_train_subsamp_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sklearn_logistic_regression.predict(X_train_subsamp_train)\n",
    "score = accuracy_score(y_train_subsamp_train, preds)\n",
    "print(f'Accuracy score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_logistic_regression.coef_, sklearn_logistic_regression.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_logistic_regression.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sklearn_logistic_regression.predict(X)\n",
    "score = accuracy_score(y, preds)\n",
    "print(f'Accuracy score: {score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'sklearn logreg': np.hstack((sklearn_logistic_regression.coef_.squeeze(), sklearn_logistic_regression.intercept_))}\n",
    "\n",
    "plot_decision_bounds(X, y, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpyMultiloss(LogRegNumpy):\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 loss_fn_name: str = 'nll',\n",
    "                 alpha: float = 0.25,\n",
    "                 gamma: float = 2.0,\n",
    "                 tolerance: float = 1e-4,\n",
    "                 early_stop: bool = False,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        super(LogRegNumpyMultiloss, self).__init__(initial_weights, initial_bias, tolerance, early_stop,\n",
    "                                                  n_startup_rounds, early_stop_rounds, random_seed)\n",
    "        self.loss_fn_name = loss_fn_name\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def loss_fn(self, y_true, y_pred):\n",
    "        if self.loss_fn_name == 'nll':\n",
    "            y_pos_loss = y_true * np.log(y_pred + self.eps)\n",
    "            y_neg_loss = (1 - y_true) * np.log(1 - y_pred + self.eps)\n",
    "            return -np.mean(y_pos_loss + y_neg_loss)\n",
    "        \n",
    "        elif self.loss_fn_name == 'hinge':\n",
    "            y_true_adjusted = np.where(y_true == 1, 1, -1)  # Convert 0 to -1 for hinge loss\n",
    "            hinge_loss = np.maximum(0, 1 - y_true_adjusted * (2 * y_pred - 1))  # Hinge formula # maximum ~ np.where\n",
    "            return np.mean(hinge_loss)\n",
    "        \n",
    "        elif self.loss_fn_name == 'focal':\n",
    "            p_t = np.where(y_true == 1, y_pred, 1 - y_pred)\n",
    "            focal_loss = -self.alpha * ((1 - p_t) ** self.gamma) * np.log(p_t + self.eps)\n",
    "            return np.mean(focal_loss)\n",
    "    \n",
    "    def gradient(self, x, y_true, y_pred):\n",
    "        if self.loss_fn_name == 'nll':\n",
    "            return np.dot((y_pred - y_true), x) / x.shape[0]\n",
    "        \n",
    "        elif self.loss_fn_name == 'hinge':\n",
    "            y_true_adjusted = np.where(y_true == 1, 1, -1)\n",
    "            margin_mask = y_true_adjusted * (2 * y_pred - 1) < 1  # Only count misclassified points\n",
    "            grad = -np.dot(margin_mask * y_true_adjusted, x) / x.shape[0]\n",
    "            return grad\n",
    "        \n",
    "        elif self.loss_fn_name == 'focal':\n",
    "            term_0 = (-self.gamma * (1 - y_pred) ** (self.gamma - 1) * np.log(y_pred + self.eps) \n",
    "                      + (1 - y_pred) ** self.gamma / (y_pred + self.eps))\n",
    "            term_0 *= y_true\n",
    "\n",
    "            term_1 = (self.gamma * y_pred ** (self.gamma - 1) * np.log(1 - y_pred + self.eps) \n",
    "                      - y_pred ** self.gamma / (1 - y_pred + self.eps))\n",
    "            term_1 *= (1 - y_true)\n",
    "\n",
    "            focal_grad = -self.alpha * (term_0 + term_1) * y_pred * (1 - y_pred)\n",
    "            return np.dot(focal_grad, x) / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "log_reg_numpy_mult_loss = LogRegNumpyMultiloss(initial_weights=initial_weights, initial_bias=initial_bias,\n",
    "                                              loss_fn_name='hinge')\n",
    "log_reg_numpy_mult_loss.fit(features=features,\n",
    "                            labels=labels,\n",
    "                            learning_rate=learning_rate,\n",
    "                            epochs=epochs)\n",
    "\n",
    "log_reg_numpy_mult_loss.weights, log_reg_numpy_mult_loss.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_mult_loss = LogRegNumpyMultiloss(loss_fn_name='hinge')\n",
    "\n",
    "log_reg_numpy_mult_loss.fit(features = X_train_subsamp_train, \n",
    "                            labels = y_train_subsamp_train, \n",
    "                            learning_rate = 1e-3, \n",
    "                            epochs = 5)\n",
    "\n",
    "log_reg_numpy_mult_loss.weights, log_reg_numpy_mult_loss.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train_subsamp_train, log_reg_numpy_mult_loss.predict(X_train_subsamp_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1: Favoring NLL Loss (overlapping classes, noisy decision boundaries)\n",
    "X_nll, y_nll = make_classification(n_samples=1000, \n",
    "                                   n_features=2,\n",
    "                                   n_informative=2, \n",
    "                                   n_redundant=0,\n",
    "                                   n_repeated=0,\n",
    "                                   n_clusters_per_class=2,\n",
    "                                   flip_y=0.12, # Add some label noise\n",
    "                                   class_sep=0.8, # Reduce class separability\n",
    "                                   random_state=SEED)\n",
    "\n",
    "# Dataset 2: Favoring Hinge Loss (clear, separable classes)\n",
    "X_hinge, y_hinge = make_classification(n_samples=1000, \n",
    "                                       n_features=2,\n",
    "                                       n_informative=2, \n",
    "                                       n_redundant=0, \n",
    "                                       n_repeated=0,\n",
    "                                       n_clusters_per_class=1,\n",
    "                                       flip_y=0.0, # No noise\n",
    "                                       class_sep=2, # High class separability\n",
    "                                       random_state=SEED)\n",
    "\n",
    "# Dataset 3: Favoring Focal Loss (clear, separable classes, imbalanced)\n",
    "X_focal, y_focal = make_classification(n_samples=1000, \n",
    "                                       n_features=2, \n",
    "                                       n_informative=2,\n",
    "                                       n_redundant=0, \n",
    "                                       n_clusters_per_class=2, \n",
    "                                       flip_y=0.01,\n",
    "                                       weights=[0.1, 0.9],\n",
    "                                       class_sep=1.0,\n",
    "                                       random_state=SEED)\n",
    "\n",
    "\n",
    "# Plotting the datasets\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 7))\n",
    "\n",
    "# Plot NLL-favoring dataset\n",
    "sns.scatterplot(x=X_nll[:, 0], y=X_nll[:, 1], hue=y_nll, palette='coolwarm', ax=axs[0])\n",
    "axs[0].set_title(\"NLL Loss Favoring Dataset\")\n",
    "\n",
    "# Plot Hinge-favoring dataset\n",
    "sns.scatterplot(x=X_hinge[:, 0], y=X_hinge[:, 1], hue=y_hinge, palette='coolwarm', ax=axs[1])\n",
    "axs[1].set_title(\"Hinge Loss Favoring Dataset\")\n",
    "\n",
    "# Plot Focal-favoring dataset\n",
    "sns.scatterplot(x=X_focal[:, 0], y=X_focal[:, 1], hue=y_focal, palette='coolwarm', ax=axs[2])\n",
    "axs[2].set_title(\"Focal Loss Favoring Dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy = LogRegNumpy(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_numpy.fit(features = X_nll, \n",
    "                  labels = y_nll, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 10000)\n",
    "\n",
    "log_reg_numpy.weights, log_reg_numpy.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_hinge = LogRegNumpyMultiloss(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6, loss_fn_name='hinge')\n",
    "\n",
    "log_reg_numpy_hinge.fit(features = X_nll, \n",
    "                        labels = y_nll, \n",
    "                        learning_rate = 1e-3, \n",
    "                        epochs = 10000)\n",
    "\n",
    "log_reg_numpy_hinge.weights, log_reg_numpy_hinge.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_focal = LogRegNumpyMultiloss(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-8, loss_fn_name='focal')\n",
    "\n",
    "log_reg_numpy_focal.fit(features = X_nll, \n",
    "                        labels = y_nll, \n",
    "                        learning_rate = 5e-3, \n",
    "                        epochs = 10000) # при большем числе эпох результат ухудшается\n",
    "\n",
    "log_reg_numpy_focal.weights, log_reg_numpy_focal.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_numpy.weights, np.array([log_reg_numpy.bias]))),\n",
    "           'Hinge': np.hstack((log_reg_numpy_hinge.weights, np.array([log_reg_numpy_hinge.bias]))),\n",
    "           'Focal': np.hstack((log_reg_numpy_focal.weights, np.array([log_reg_numpy_focal.bias])))}\n",
    "\n",
    "plot_decision_bounds(X_nll, \n",
    "                     y_nll, \n",
    "                     weights,\n",
    "                     dict(palette='coolwarm'),\n",
    "                     title='NLL loss favoring dataset')\n",
    "\n",
    "print(f'NLL loss accuracy score: {accuracy_score(y_nll, log_reg_numpy.predict(X_nll)):.4f}')\n",
    "print(f'Hinge loss accuracy score: {accuracy_score(y_nll, log_reg_numpy_hinge.predict(X_nll)):.4f}')\n",
    "print(f'Focal loss accuracy score: {accuracy_score(y_nll, log_reg_numpy_focal.predict(X_nll)):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy = LogRegNumpy(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_numpy.fit(features = X_hinge, \n",
    "                  labels = y_hinge, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 10000)\n",
    "\n",
    "log_reg_numpy.weights, log_reg_numpy.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_hinge = LogRegNumpyMultiloss(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6, loss_fn_name='hinge')\n",
    "\n",
    "log_reg_numpy_hinge.fit(features = X_hinge, \n",
    "                        labels = y_hinge, \n",
    "                        learning_rate = 1e-3, \n",
    "                        epochs = 10000)\n",
    "\n",
    "log_reg_numpy_hinge.weights, log_reg_numpy_hinge.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_focal = LogRegNumpyMultiloss(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-8, loss_fn_name='focal')\n",
    "\n",
    "log_reg_numpy_focal.fit(features = X_hinge, \n",
    "                        labels = y_hinge, \n",
    "                        learning_rate = 1e-2, \n",
    "                        epochs = 10000)\n",
    "\n",
    "log_reg_numpy_focal.weights, log_reg_numpy_focal.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_numpy.weights, np.array([log_reg_numpy.bias]))),\n",
    "           'Hinge': np.hstack((log_reg_numpy_hinge.weights, np.array([log_reg_numpy_hinge.bias]))),\n",
    "           'Focal': np.hstack((log_reg_numpy_focal.weights, np.array([log_reg_numpy_focal.bias])))}\n",
    "\n",
    "plot_decision_bounds(X_hinge, \n",
    "                     y_hinge, \n",
    "                     weights,\n",
    "                     dict(palette='coolwarm'),\n",
    "                     title='Hinge loss favoring dataset')\n",
    "\n",
    "print(f'NLL loss accuracy score: {accuracy_score(y_hinge, log_reg_numpy.predict(X_hinge)):.4f}')\n",
    "print(f'Hinge loss accuracy score: {accuracy_score(y_hinge, log_reg_numpy_hinge.predict(X_hinge)):.4f}')\n",
    "print(f'Focal loss accuracy score: {accuracy_score(y_hinge, log_reg_numpy_focal.predict(X_hinge)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Focal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy = LogRegNumpy(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6)\n",
    "\n",
    "log_reg_numpy.fit(features = X_focal, \n",
    "                  labels = y_focal, \n",
    "                  learning_rate = 1e-3, \n",
    "                  epochs = 15000)\n",
    "\n",
    "log_reg_numpy.weights, log_reg_numpy.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_hinge = LogRegNumpyMultiloss(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6, loss_fn_name='hinge')\n",
    "\n",
    "log_reg_numpy_hinge.fit(features = X_focal, \n",
    "                        labels = y_focal, \n",
    "                        learning_rate = 1e-3, \n",
    "                        epochs = 15000)\n",
    "\n",
    "log_reg_numpy_hinge.weights, log_reg_numpy_hinge.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_numpy_focal = LogRegNumpyMultiloss(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-8, loss_fn_name='focal')\n",
    "\n",
    "log_reg_numpy_focal.fit(features = X_focal, \n",
    "                        labels = y_focal, \n",
    "                        learning_rate = 1e-2, \n",
    "                        epochs = 15000)\n",
    "\n",
    "log_reg_numpy_focal.weights, log_reg_numpy_focal.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'NLL': np.hstack((log_reg_numpy.weights, np.array([log_reg_numpy.bias]))),\n",
    "           'Hinge': np.hstack((log_reg_numpy_hinge.weights, np.array([log_reg_numpy_hinge.bias]))),\n",
    "           'Focal': np.hstack((log_reg_numpy_focal.weights, np.array([log_reg_numpy_focal.bias])))}\n",
    "\n",
    "plot_decision_bounds(X_focal, \n",
    "                     y_focal, \n",
    "                     weights,\n",
    "                     dict(palette='coolwarm'),\n",
    "                     title='Focal loss favoring dataset')\n",
    "\n",
    "for scorer in (accuracy_score, f1_score):\n",
    "    print(f'NLL loss {scorer.__name__}: {scorer(y_focal, log_reg_numpy.predict(X_focal)):.4f}')\n",
    "    print(f'Hinge loss {scorer.__name__}: {scorer(y_focal, log_reg_numpy_hinge.predict(X_focal)):.4f}')\n",
    "    print(f'Focal loss {scorer.__name__}: {scorer(y_focal, log_reg_numpy_focal.predict(X_focal)):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReduction:\n",
    "    def __init__(self, params_history, n_components=2, seed=None):\n",
    "\n",
    "        self.params_history = np.vstack([p.flatten() for p in params_history])\n",
    "        self.n_components = n_components\n",
    "        self.seed = seed\n",
    "\n",
    "    def reduce_params(self):\n",
    "        pca = PCA(n_components=self.n_components, random_state=self.seed)\n",
    "        path_2d = pca.fit_transform(self.params_history)\n",
    "        reduced_dirs = pca.components_  # The PCA directions\n",
    "\n",
    "        return path_2d, reduced_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 50\n",
    "MARGIN = 0.5\n",
    "\n",
    "class LossGrid:\n",
    "    \"\"\"The loss grid class that holds the values of 2D slice from the loss landscape.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optim_path,\n",
    "        model,\n",
    "        data,\n",
    "        path_2d,\n",
    "        directions,\n",
    "        res=RES,\n",
    "        tqdm_disable=False,\n",
    "    ):\n",
    "        \"\"\"Init a loss grid object.\n",
    "\n",
    "        Args:\n",
    "            optim_path: The full-dimensional flattened parameters during training.\n",
    "            model: The model for loss evaluation.\n",
    "            data: The data module for loss evaluation.\n",
    "            path_2d: The list of 2D coordinates.\n",
    "            directions: The 2D directions/axes.\n",
    "            res (optional): Resolution of the grid. Defaults to RES.\n",
    "            tqdm_disable (optional): Whether to disable progress bar. Defaults to False.\n",
    "            save_grid (optional): Whether to save the grid to file. Defaults to True.\n",
    "            load_grid (optional): Whether to load from file. Defaults to False.\n",
    "            filepath (optional): Defaults to \"./checkpoints/lossgrid.p\".\n",
    "        \"\"\"\n",
    "        self.dir0, self.dir1 = directions\n",
    "        self.path_2d = path_2d\n",
    "        self.path_nd = optim_path\n",
    "        self.optim_point = optim_path[-1]\n",
    "        self.optim_point_2d = path_2d[-1]\n",
    "\n",
    "        alpha = self._compute_stepsize(res)\n",
    "        self.params_grid = self.build_params_grid(res, alpha)\n",
    "\n",
    "        self.loss_values_2d, self.argmin, self.loss_min = self.compute_loss_2d(\n",
    "            model, data, tqdm_disable=tqdm_disable\n",
    "        )\n",
    "        self.loss_values_optimizer = self.compute_optimizer_loss(model, data, tqdm_disable)\n",
    "\n",
    "        self.loss_values_log_2d = np.log(self.loss_values_2d)\n",
    "        self.loss_values_optimizer_log = np.log(self.loss_values_optimizer)\n",
    "        self.coords = self._convert_coords(res, alpha)\n",
    "        # True optim in loss grid\n",
    "        self.true_optim_point = self.indices_to_coords(self.argmin, res, alpha)\n",
    "\n",
    "    def build_params_grid(self, res, alpha):\n",
    "        \"\"\"\n",
    "        Produce the grid for the contour plot.\n",
    "\n",
    "        Start from the optimal point, span directions of the pca result with\n",
    "        stepsize alpha, resolution res.\n",
    "        \"\"\"\n",
    "        grid = []\n",
    "        for i in range(-res, res):\n",
    "            row = []\n",
    "            for j in range(-res, res):\n",
    "                w_new = (\n",
    "                    self.optim_point\n",
    "                    + i * alpha * self.dir0\n",
    "                    + j * alpha * self.dir1\n",
    "                )\n",
    "                row.append(w_new)\n",
    "            grid.append(row)\n",
    "        assert (grid[res][res] == self.optim_point).all()\n",
    "        return grid\n",
    "\n",
    "    def compute_optimizer_loss(self, model, data, tqdm_disable):\n",
    "        X, y = data\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        loss = []\n",
    "\n",
    "        for i in tq.tqdm(range(self.path_nd.shape[0]), disable=tqdm_disable):\n",
    "            y_pred = model.forward(X, self.path_nd[i])\n",
    "            loss_val = model.loss_fn(y, y_pred)\n",
    "            loss.append(loss_val)\n",
    "        return np.array(loss)\n",
    "\n",
    "    def compute_loss_2d(self, model, data, tqdm_disable=False):\n",
    "        \"\"\"Compute loss values for each weight vector in grid for the model and data.\"\"\"\n",
    "        X, y = data\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        loss_2d = []\n",
    "        n = len(self.params_grid)\n",
    "        m = len(self.params_grid[0])\n",
    "        loss_min = float(\"inf\")\n",
    "        argmin = ()\n",
    "        print(\"Generating loss values for the contour plot...\")\n",
    "        with tq.tqdm(total=n * m, disable=tqdm_disable) as pbar:\n",
    "            for i in range(n):\n",
    "                loss_row = []\n",
    "                for j in range(m):\n",
    "                    w_ij = self.params_grid[i][j]\n",
    "                    # Load flattened weight vector into model\n",
    "                    y_pred = model.forward(X, w_ij)\n",
    "                    loss_val = model.loss_fn(y, y_pred)\n",
    "                    if loss_val < loss_min:\n",
    "                        loss_min = loss_val\n",
    "                        argmin = (i, j)\n",
    "                    loss_row.append(loss_val)\n",
    "                    pbar.update(1)\n",
    "                loss_2d.append(loss_row)\n",
    "        # This transpose below is very important for a correct contour plot because\n",
    "        # originally in loss_2d, dir1 (y) is row-direction, dir0 (x) is column\n",
    "        loss_2darray = np.array(loss_2d).T\n",
    "        print(\"\\nLoss values generated.\")\n",
    "        return loss_2darray, argmin, loss_min\n",
    "\n",
    "    def _convert_coord(self, i, ref_point_coord, alpha):\n",
    "        \"\"\"\n",
    "        Convert from integer index to the coordinate value.\n",
    "\n",
    "        Given a reference point coordinate (1D), find the value i steps away with\n",
    "        step size alpha.\n",
    "        \"\"\"\n",
    "        return i * alpha + ref_point_coord\n",
    "\n",
    "    def _convert_coords(self, res, alpha):\n",
    "        \"\"\"\n",
    "        Convert the coordinates from (i, j) indices to (x, y) values.\n",
    "\n",
    "        Remember that for PCA, the coordinates have unit vectors as the top 2 PCs.\n",
    "\n",
    "        Original path_2d has PCA output, i.e. the 2D projections of each W step\n",
    "        onto the 2D space spanned by the top 2 PCs.\n",
    "        We need these steps in (i, j) terms with unit vectors\n",
    "        reduced_w1 = (1, 0) and reduced_w2 = (0, 1) in the 2D space.\n",
    "\n",
    "        We center the plot on optim_point_2d, i.e.\n",
    "        let center_2d = optim_point_2d\n",
    "\n",
    "        ```\n",
    "        i = (x - optim_point_2d[0]) / alpha\n",
    "        j = (y - optim_point_2d[1]) / alpha\n",
    "\n",
    "        i.e.\n",
    "\n",
    "        x = i * alpha + optim_point_2d[0]\n",
    "        y = j * alpha + optim_point_2d[1]\n",
    "        ```\n",
    "\n",
    "        where (x, y) is the 2D points in path_2d from PCA. Again, the unit\n",
    "        vectors are reduced_w1 and reduced_w2.\n",
    "        Return the grid coordinates in terms of (x, y) for the loss values\n",
    "        \"\"\"\n",
    "        converted_coord_xs = []\n",
    "        converted_coord_ys = []\n",
    "        for i in range(-res, res):\n",
    "            x = self._convert_coord(i, self.optim_point_2d[0], alpha)\n",
    "            y = self._convert_coord(i, self.optim_point_2d[1], alpha)\n",
    "            converted_coord_xs.append(x)\n",
    "            converted_coord_ys.append(y)\n",
    "        return np.array(converted_coord_xs), np.array(converted_coord_ys)\n",
    "\n",
    "    def indices_to_coords(self, indices, res, alpha):\n",
    "        \"\"\"Convert the (i, j) indices to (x, y) coordinates.\n",
    "\n",
    "        Args:\n",
    "            indices: (i, j) indices to convert.\n",
    "            res: Resolution.\n",
    "            alpha: Step size.\n",
    "\n",
    "        Returns:\n",
    "            The (x, y) coordinates in the projected 2D space.\n",
    "        \"\"\"\n",
    "        grid_i, grid_j = indices\n",
    "        i, j = grid_i - res, grid_j - res\n",
    "        x = i * alpha + self.optim_point_2d[0]\n",
    "        y = j * alpha + self.optim_point_2d[1]\n",
    "        return x, y\n",
    "\n",
    "    def _compute_stepsize(self, res):\n",
    "        dist_2d = self.path_2d[-1] - self.path_2d[0]\n",
    "        dist = (dist_2d[0] ** 2 + dist_2d[1] ** 2) ** 0.5\n",
    "        return dist * (1 + MARGIN) / res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegNumpyMiniBatch(LogRegNumpyMultiloss):\n",
    "\n",
    "    def __init__(self, \n",
    "                 initial_weights: list[list[float]] = [], \n",
    "                 initial_bias: list[float] = [],\n",
    "                 loss_fn_name: str = 'nll',\n",
    "                 alpha: float = 0.25,\n",
    "                 gamma: float = 2.0,\n",
    "                 tolerance: float = 1e-4,\n",
    "                 early_stop: bool = False,\n",
    "                 n_startup_rounds: int = 50,\n",
    "                 early_stop_rounds: int = 50,\n",
    "                 random_seed: int = SEED):\n",
    "        \n",
    "        super(LogRegNumpyMiniBatch, self).__init__(initial_weights, initial_bias, loss_fn_name, alpha, gamma,\n",
    "                                                   tolerance, early_stop, n_startup_rounds, early_stop_rounds, random_seed)\n",
    "    \n",
    "    def fit(self, \n",
    "            features: list[list[float]], \n",
    "            labels: list[int],\n",
    "            batch_size: int = 1,\n",
    "            learning_rate: float = 1e-3, \n",
    "            epochs: int = 100,\n",
    "            return_weights_history: bool = False) -> None | list[list[float]]:\n",
    "\n",
    "        X = np.array(features).squeeze() if not isinstance(features, np.ndarray) else deepcopy(features)\n",
    "        X = self.add_ones(X)\n",
    "        y = np.array(labels).squeeze() if not isinstance(labels, np.ndarray) else deepcopy(labels)\n",
    "\n",
    "        self.init_weights(X)\n",
    "        \n",
    "        w = np.hstack([self.bias, self.weights])\n",
    "\n",
    "        loss_values = []\n",
    "        if return_weights_history:\n",
    "            weights_values = []\n",
    "        rng_train = np.random.default_rng(seed=self.random_seed)\n",
    "        num_batches = np.ceil(X.shape[0] / batch_size)\n",
    "        add_to_full_batch = int(batch_size * num_batches - X.shape[0])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            train_loss = 0\n",
    "\n",
    "            idx_set = np.arange(0, X.shape[0], 1)\n",
    "            idx_set = np.hstack([idx_set, rng_train.choice(idx_set, add_to_full_batch, replace=False)])\n",
    "            rng_train.shuffle(idx_set)\n",
    "\n",
    "            for idx in range(0, X.shape[0] - batch_size, batch_size):\n",
    "                X_batch = X[idx_set[idx : idx + batch_size]]\n",
    "                y_batch = y[idx_set[idx : idx + batch_size]]\n",
    "                y_pred = self.forward(X_batch, w)\n",
    "                loss = self.loss_fn(y_batch, y_pred)\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                grad = self.gradient(X_batch, y_batch, y_pred)\n",
    "                w -= learning_rate * grad\n",
    "            \n",
    "            train_loss /= num_batches\n",
    "            loss_values.append(train_loss)\n",
    "            if return_weights_history:\n",
    "                weights_values.append(w.copy())\n",
    "\n",
    "        self.weights = w[1:]\n",
    "        self.bias = w[0]\n",
    "        self.loss_values = loss_values\n",
    "\n",
    "        if return_weights_history:\n",
    "            return np.array(weights_values)\n",
    "\n",
    "    def predict(self, features: list[list[float]], batch_size: int = 32):\n",
    "        X = np.array(features).squeeze() if not isinstance(features, np.ndarray) else deepcopy(features)\n",
    "        X = self.add_ones(X)\n",
    "\n",
    "        w = np.hstack([self.bias, self.weights])\n",
    "        \n",
    "        idx_set = np.arange(0, X.shape[0], 1)\n",
    "        probs = np.empty(0)\n",
    "        for idx in range(0, X.shape[0] - batch_size, batch_size):\n",
    "            X_batch = X[idx_set[idx : idx + batch_size]]\n",
    "            probs_batch = self.forward(X_batch, w)\n",
    "            probs = np.append(probs, probs_batch)\n",
    "        \n",
    "        probs = np.append(probs, self.forward(X[idx + batch_size : X.shape[0]], w))\n",
    "\n",
    "        return np.where(probs >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multiple_models(X: list[list[float]], y: list[float], model_classes: list[LogRegNumpy], scorers: list[callable],\n",
    "                          model_params: list[dict], fit_params: list[dict], names: list[str], return_models: bool = False):\n",
    "\n",
    "    weights, loss, scores, weights_history, models = dict(), dict(), dict(), dict(), dict()\n",
    "\n",
    "    for model_class, m_params, f_params, name in zip(model_classes, model_params, fit_params, names):\n",
    "        print('Training model', name)\n",
    "        model = model_class(**m_params)\n",
    "        if 'return_weights_history' in f_params:\n",
    "            if f_params['return_weights_history']:\n",
    "                weights_history[name] = model.fit(features=X, labels=y, **f_params)\n",
    "        else:\n",
    "            model.fit(features=X, labels=y, **f_params)\n",
    "\n",
    "        weights[name] = np.hstack((model.weights, np.array([model.bias])))\n",
    "        loss[name] = model.loss_values\n",
    "\n",
    "        scores[name] = dict()\n",
    "        for scorer in scorers:\n",
    "            scores[name][scorer.__name__] = scorer(y, model.predict(X))\n",
    "\n",
    "        if return_models:\n",
    "            models[name] = model\n",
    "\n",
    "    if return_models:\n",
    "        if weights_history:\n",
    "            return weights, loss, scores, weights_history, models\n",
    "        return weights, loss, scores, models\n",
    "    elif weights_history:\n",
    "        return weights, loss, scores, weights\n",
    "    else:\n",
    "        return weights, loss, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Костыль section\n",
    "# NLL_tmp = LogRegNumpyMultiloss(random_seed=2147483647)\n",
    "# NLL_tmp.fit(X_nll, y_nll, epochs=0)\n",
    "\n",
    "# init_weights, init_bias = NLL_tmp.weights, NLL_tmp.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [\n",
    "    LogRegNumpyMultiloss,\n",
    "    LogRegNumpyMiniBatch,\n",
    "    LogRegNumpyMiniBatch,\n",
    "] * 3\n",
    "\n",
    "scorers = [\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "]\n",
    "\n",
    "# !!! Даже с одним сидом я не могу сделать веса воспроизводимыми (почему?)\n",
    "model_params = [\n",
    "    dict(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6, loss_fn_name='nll'),\n",
    "] * 3 + [\n",
    "    dict(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-6, loss_fn_name='hinge'),   \n",
    "] * 3 + [\n",
    "    dict(random_seed=2147483647, n_startup_rounds=100, tolerance=1e-8, loss_fn_name='focal'),\n",
    "] * 3\n",
    "\n",
    "fit_params = [\n",
    "    dict(learning_rate=1e-3, epochs=10000, return_weights_history=True),\n",
    "    dict(batch_size=8, learning_rate=1e-3, epochs=10000, return_weights_history=True),\n",
    "    dict(batch_size=32, learning_rate=1e-3, epochs=10000, return_weights_history=True),\n",
    "\n",
    "    dict(learning_rate=1e-3, epochs=10000, return_weights_history=True),\n",
    "    dict(batch_size=8, learning_rate=1e-3, epochs=10000, return_weights_history=True),\n",
    "    dict(batch_size=32, learning_rate=1e-3, epochs=10000, return_weights_history=True),\n",
    "\n",
    "    dict(learning_rate=5e-3, epochs=10000, return_weights_history=True),\n",
    "    dict(batch_size=8, learning_rate=5e-3, epochs=10000, return_weights_history=True),\n",
    "    dict(batch_size=32, learning_rate=5e-3, epochs=10000, return_weights_history=True),\n",
    "]\n",
    "\n",
    "model_names = (['NLL', 'NLL (batch 8)', 'NLL (batch 32)'] + \n",
    "               ['Hinge', 'Hinge (batch 8)', 'Hinge (batch 32)'] + \n",
    "               ['Focal', 'Focal (batch 8)', 'Focal (batch 32)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, loss, scores, weights_history, models = train_multiple_models(X_nll, y_nll, \n",
    "                                                                       model_classes, \n",
    "                                                                       scorers,\n",
    "                                                                       model_params, \n",
    "                                                                       fit_params, \n",
    "                                                                       model_names, \n",
    "                                                                       True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, s in scores.items():\n",
    "#     for score_name, score_value in s.items():\n",
    "#         print(f'{name} {score_name}: {score_value:.4f}')\n",
    "\n",
    "df_comp = pd.DataFrame(scores)\n",
    "df_comp = df_comp.transpose()\n",
    "display(df_comp.style.background_gradient(cmap='coolwarm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_bounds(X_nll, y_nll, dict(islice(weights.items(), 3)), dict(palette='coolwarm'), title='NLL loss favoring dataset')\n",
    "plot_decision_bounds(X_nll, y_nll, dict(islice(weights.items(), 3, 6)), dict(palette='coolwarm'), title='NLL loss favoring dataset')\n",
    "plot_decision_bounds(X_nll, y_nll, dict(islice(weights.items(), 6, 9)), dict(palette='coolwarm'), title='NLL loss favoring dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(dict(islice(loss.items(), 3)), 'matplotlib')\n",
    "plot_losses(dict(islice(loss.items(), 3, 6)), 'matplotlib', palette=palette[:3])\n",
    "plot_losses(dict(islice(loss.items(), 6, 9)), 'matplotlib', palette=palette[3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_params(weights_history: dict[str: list[list[float]]]):\n",
    "    result = dict()\n",
    "    for name, history in weights_history.items():\n",
    "        dim_reducer = DimensionalityReduction(history, 2, SEED)\n",
    "        result[name] = dim_reducer.reduce_params()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_params = get_reduced_params(weights_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grids(weights_history: dict[str: list[list[float]]],\n",
    "              models: dict[str: LogRegNumpyMultiloss|LogRegNumpyMiniBatch],\n",
    "              data: tuple[list[list[float]], list[int]],\n",
    "              reduced_params: dict[str: Sequence[list[list[float]]], list[list[float]]], \n",
    "              res: int):\n",
    "    \n",
    "    result = dict()\n",
    "    for name in reduced_params.keys():\n",
    "        result[name] = LossGrid(weights_history[name], models[name], data, reduced_params[name][0], reduced_params[name][1], res, True)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grids = get_grids(weights_history, models, (X_nll, y_nll), reduced_params, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(reduced_params, loss_grid, coords, true_optim_point, animate=False, step=20, label='', color=palette[1]):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    coords_x, coords_y = coords\n",
    "    ax.contourf(coords_x, coords_y, loss_grid, levels=35, alpha=0.9, cmap=\"YlGnBu\")\n",
    "\n",
    "    ax.plot(true_optim_point[0], true_optim_point[1], 'k*', markersize=8, label=\"True Optimum\")\n",
    "    ax.text(true_optim_point[0] + 0.1, true_optim_point[1] + 0.1,\n",
    "            f'({true_optim_point[0]:.2f}, {true_optim_point[1]:.2f})',\n",
    "            fontsize=10, color='black')\n",
    "\n",
    "    if not animate:\n",
    "        w1s = [step[0] for step in reduced_params]\n",
    "        w2s = [step[1] for step in reduced_params]\n",
    "        ax.plot(w1s, w2s, lw=2, label=label, color=color)\n",
    "    else:\n",
    "        line, = ax.plot([], [], lw=2, label=label, color=color)\n",
    "        w1s = [step[0] for step in reduced_params[::step]]\n",
    "        w2s = [step[1] for step in reduced_params[::step]]\n",
    "\n",
    "        def update(i):\n",
    "            line.set_xdata(w1s[:i])\n",
    "            line.set_ydata(w2s[:i])\n",
    "            return line\n",
    "\n",
    "        anim = animation.FuncAnimation(fig, update, frames=len(w1s), interval=100, blit=False, repeat=False)\n",
    "\n",
    "    ax.legend()\n",
    "    plt.title(\"Optimizer Path Comparison\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(model_names):\n",
    "    plot_contour(reduced_params[name][0], \n",
    "                 loss_grids[name].loss_values_log_2d, \n",
    "                 loss_grids[name].coords, \n",
    "                 loss_grids[name].true_optim_point, False, 50, label=name, color=palette[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3d_surface(steps, loss_grid, coords, true_optim_point, title='', label='', color=palette[0], plot_engine='plotly'):\n",
    "    coords_x, coords_y = coords\n",
    "\n",
    "    w1s = [step[0] for step in steps]\n",
    "    w2s = [step[1] for step in steps]\n",
    "    ls  = [step[2] for step in steps]\n",
    "\n",
    "    if plot_engine == 'plotly':\n",
    "        # Add the loss surface\n",
    "        surface = go.Surface(z=loss_grid, x=coords_x, y=coords_y, colorscale=\"YlGnBu\", opacity=0.7, )\n",
    "\n",
    "        # Add the optimizer path\n",
    "        \n",
    "        optim_path = go.Scatter3d(x=w1s, y=w2s, z=ls, mode='lines', line=dict(color=color, width=4), name=label)\n",
    "\n",
    "        # Mark the true optimum point\n",
    "        true_optim = go.Scatter3d(x=[true_optim_point[0]], y=[true_optim_point[1]], z=[loss_grid.min()],\n",
    "                                mode='markers', marker=dict(color='black', size=10, symbol='cross'), name='True Optimum')\n",
    "\n",
    "        layout = go.Layout(title=title,\n",
    "                        scene=dict(xaxis_title='w0', yaxis_title='w1', zaxis_title='Loss'),\n",
    "                        legend=dict(yanchor=\"top\",y=0.99,xanchor=\"left\",x=0.01),\n",
    "                        height=800, width=800)\n",
    "\n",
    "        fig = go.Figure(data=[surface, optim_path, true_optim], layout=layout)\n",
    "        fig.show()\n",
    "        \n",
    "    elif plot_engine == 'matplotlib':\n",
    "        \n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        X, Y = np.meshgrid(coords_x, coords_y)\n",
    "        ax.plot_surface(X, Y, loss_grid, cmap='YlGnBu', alpha=0.7)\n",
    "        ax.plot(w1s, w2s, ls, color=color, linewidth=2, label=label)\n",
    "        ax.scatter(true_optim_point[0], true_optim_point[1], np.min(loss_grid), color='black', s=100, marker='x', label='True Optimum')\n",
    "\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel(\"w0\")\n",
    "        ax.set_ylabel(\"w1\")\n",
    "        ax.set_zlabel(\"Loss\")\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, name in enumerate(model_names):\n",
    "    path_3d = np.c_[reduced_params[name][0], loss_grids[name].loss_values_optimizer_log]\n",
    "    plot_3d_surface(path_3d, \n",
    "                    loss_grids[name].loss_values_log_2d, \n",
    "                    loss_grids[name].coords, \n",
    "                    loss_grids[name].true_optim_point,\n",
    "                    '',\n",
    "                    name,\n",
    "                    palette[i],\n",
    "                    'plotly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expenv python 3.11.5",
   "language": "python",
   "name": "expenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
